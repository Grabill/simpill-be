{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medspacy_pyrush', 'medspacy_target_matcher', 'medspacy_context']\n"
     ]
    }
   ],
   "source": [
    "categories = [\"Head and Neck\", \"Respiratory\", \"Cardiovascular\", \"Gastrointestinal\", \"Neurological\", \"Genitourinary\", \"Skin\", \"General\"]\n",
    "\n",
    "import medspacy\n",
    "from medspacy.ner import TargetRule\n",
    "from medspacy.visualization import visualize_ent\n",
    "import json\n",
    "\n",
    "# Load medspacy model\n",
    "nlp = medspacy.load()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Load rules from JSON file\n",
    "with open(\"symp_rules.json\", \"r\") as f:\n",
    "    symp_rules_json = json.load(f)\n",
    "\n",
    "target_rules = []\n",
    "\n",
    "for category in categories:\n",
    "    # Create a TargetRule object\n",
    "    for rule_data in symp_rules_json[category]:\n",
    "        if 'pattern' in rule_data:\n",
    "            target_rule = TargetRule(rule_data['phrase'], rule_data['type'], pattern=rule_data['pattern'])\n",
    "        else:\n",
    "            target_rule = TargetRule(rule_data['phrase'], rule_data['type'])\n",
    "    \n",
    "        # Add the TargetRule object to the list\n",
    "        target_rules.append(target_rule)\n",
    "\n",
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")\n",
    "target_matcher.add(target_rules)\n",
    "\n",
    "# import splm['details'] + splm['uses'] from splm_cleaned.json\n",
    "with open(\"../cleaned_data/splm_cleaned.json\", \"r\") as f:\n",
    "    splms = json.load(f)\n",
    "\n",
    "splms = [str(splm['overview']) + str(splm['uses']) for splm in splms]\n",
    "unqiue_symptoms = set()\n",
    "\n",
    "# extract symptoms to a file\n",
    "with open(\"../output/symptoms.txt\", \"w\") as f:\n",
    "    # loop through each splm\n",
    "    for text in splms:\n",
    "        # process the text using medspacy\n",
    "        doc = nlp(text)\n",
    "        # extract symptoms\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"SYMPTOM\":\n",
    "                unqiue_symptoms.add(str(ent.text).lower())\n",
    "\n",
    "# sort symptoms\n",
    "unqiue_symptoms = sorted(unqiue_symptoms)\n",
    "\n",
    "# write symptoms to file\n",
    "with open(\"../output/symptoms.txt\", \"w\") as f:\n",
    "    for symptom in unqiue_symptoms:\n",
    "        f.write(f\"{symptom}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"../word2vec/splm_word2vec.model\")\n",
    "\n",
    "# Target combination of words\n",
    "target_combination = \"shortness of breath\"\n",
    "\n",
    "# Tokenize the target combination\n",
    "target_tokens = target_combination.split()\n",
    "\n",
    "# Convert tokens to word vectors\n",
    "target_vectors = [word2vec_model.wv[token] for token in target_tokens]\n",
    "\n",
    "# Combine vectors of individual tokens (e.g., by averaging)\n",
    "combined_vector = sum(target_vectors) / len(target_vectors)\n",
    "\n",
    "# Compute similarity scores with all other vectors in the dataset\n",
    "similarity_scores = cosine_similarity([combined_vector], word2vec_model.wv.vectors)\n",
    "\n",
    "# Rank combinations based on similarity scores\n",
    "similar_combinations_indices = similarity_scores.argsort(axis=1)[:, ::-1]\n",
    "\n",
    "for index in similar_combinations_indices[0, 1:]:\n",
    "    print(word2vec_model.wv.index_to_key[index])\n",
    "\n",
    "# Retrieve top similar combinations\n",
    "# top_similar_combinations = [word2vec_model.wv.index_to_key[index] for index in similar_combinations_indices[:, 1:]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
