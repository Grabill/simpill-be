{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary modules\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from json file\n",
    "with open('splm_cleaned.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "descriptions = [str(d['overview']).lower() for d in data]\n",
    "\n",
    "def tokenize_lower_and_remove_punctuation(text):\n",
    "    tokens = word_tokenize(text, preserve_line=False)\n",
    "    return [token.lower() for token in tokens if token.isalnum()]\n",
    "\n",
    "# Step 1: Preprocess and vectorize the description paragraphs\n",
    "# descriptions = [\n",
    "#                 'Skip-Gram predicts context words from a target word, and CBOW predicts a target word based on its context. While Skip-Gram frequently performs better for infrequent words, CBOW is faster and typically performs better with frequent words.',\n",
    "#                 'Using a large corpus, word embeddings are trained by modifying vector representations in response to how well the model predicts target or context words.',\n",
    "#                 'The similarity between two vectors in an inner product space is measured by cosine similarity. It finds whether two vectors are roughly pointing in the same direction by measuring the cosine of the angle between them. In text analysis, it is frequently used to gauge document similarity.'\n",
    "#             ]\n",
    "\n",
    "# Apply lowercase and whitespace split only\n",
    "# descriptions = [d.lower().split() for d in descriptions]\n",
    "descriptions = [tokenize_lower_and_remove_punctuation(d) for d in descriptions]\n",
    "\n",
    "\n",
    "# print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=descriptions, vector_size=250, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Create a dictionary mapping words to their vector representations\n",
    "word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "\n",
    "# Create a vector representation for each description paragraph\n",
    "description_vectors = [np.mean([word_vectors[word] for word in desc if word in word_vectors], axis=0) for desc in descriptions]\n",
    "\n",
    "# Step 2: Preprocess and vectorize the input keywords\n",
    "def process_keywords(keywords):\n",
    "    # Same preprocessing procedure as that of the descriptions\n",
    "    return keywords.lower().split()\n",
    "\n",
    "def vectorize_keywords(keywords):\n",
    "    return np.mean([word_vectors[word] for word in keywords if word in word_vectors], axis=0)\n",
    "\n",
    "# Step 3: Compare the vector of the input keywords to the vectors of the description paragraphs using Cosine similarity metric\n",
    "def find_best_match(keywords_vector):\n",
    "    similarities = cosine_similarity([keywords_vector], description_vectors)\n",
    "    best_match_index = np.argmax(similarities)\n",
    "    return best_match_index\n",
    "\n",
    "# Step 4: Return the description paragraph that has the highest similarity score\n",
    "def find_object(keywords):\n",
    "    keywords = process_keywords(keywords)\n",
    "    print(\"Keywords: \", keywords)\n",
    "    keywords_vector = vectorize_keywords(keywords)\n",
    "    best_match_index = find_best_match(keywords_vector)\n",
    "    # print(word_vectors)\n",
    "    print(\"Description vector: \", len(description_vectors))\n",
    "    print(\"Keywords vector: \", len(keywords_vector))\n",
    "    return best_match_index # Return the index to the best-matched paragraph\n",
    "\n",
    "res = find_object(\"fever cough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result in json format\n",
    "print(json.dumps(data[res], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=descriptions, vector_size=250, window=5, min_count=1, workers=4, sg=1)\n",
    "model.save(\"splm_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asthma', 0.9841917753219604), ('cough', 0.9793998003005981), ('constipation', 0.9775192737579346), ('diarrhea', 0.9773197770118713), ('arthritis', 0.972391664981842), ('hay', 0.9705657958984375), ('gallbladder', 0.9704510569572449), ('colds', 0.9701707363128662), ('migraine', 0.9696513414382935), ('rheumatoid', 0.9676762819290161)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Load the model\n",
    "model = Word2Vec.load(\"splm_word2vec.model\")\n",
    "\n",
    "# Top 10 most similar words to \"fever\"\n",
    "sims = model.wv.most_similar(\"fever\", topn=10)\n",
    "print(sims)\n",
    "\n",
    "# Create a dictionary mapping words to their vector representations\n",
    "word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "\n",
    "# print(\"Number of words in the vocabulary: \", len(word_vectors))\n",
    "\n",
    "# # print(\"Description: \", descriptions[0])\n",
    "\n",
    "# # Vectorize each word in the description paragraphs\n",
    "description_word_vectors = [[word_vectors[word] for word in word_tokenize(desc) if word in word_vectors] for desc in descriptions] # shape: (num_descriptions, num_words, vector_size)\n",
    "\n",
    "def vectorize_keywords(keywords):\n",
    "    keywords = word_tokenize(keywords.lower())\n",
    "    return [word_vectors[word] for word in keywords if word in word_vectors]\n",
    "\n",
    "def find_best_match(keywords_vector):\n",
    "    print(len(keywords_vector), len(description_word_vectors[0]))\n",
    "    return 0, 0\n",
    "    # similarities = [cosine_similarity([keywords_vector], [desc_vec])[0][0] for desc_vec in description_word_vectors]\n",
    "    # best_match_index = np.argmax(similarities)\n",
    "    # return best_match_index, similarities[best_match_index]\n",
    "\n",
    "def find_object(keywords):\n",
    "    keywords_vector = vectorize_keywords(keywords) # shape: (num_keywords, vector_size)\n",
    "    print(\"Keywords vector: \", keywords_vector)\n",
    "    best_match_index, best_similarity = find_best_match(keywords_vector)\n",
    "    return best_match_index, best_similarity\n",
    "\n",
    "res, _ = find_object(\"fever cough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(data[res], indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
